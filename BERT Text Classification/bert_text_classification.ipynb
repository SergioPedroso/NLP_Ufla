{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sergi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf                # Tensor operations and GPU/TPU support\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# Hugging Face Transformers\n",
    "from transformers import AutoTokenizer,DataCollatorWithPadding, TFAutoModelForSequenceClassification, create_optimizer, pipeline\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "# Datasets & evaluation\n",
    "from datasets import load_dataset      # ü§ó¬†Datasets loader\n",
    "import evaluate                        # Unified metrics API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** HateBR from ü§ó‚ÄØDatasets\n",
    "[https://huggingface.co/datasets/ruanchaves/hatebr](https://huggingface.co/datasets/ruanchaves/hatebr)\n",
    "\n",
    "HateBR comprises Brazilian‚ÄëPortuguese political comments scraped from Instagram and labeled for offensive language and hate speech.\n",
    "\n",
    "* **Size:** 7,000 annotated documents\n",
    "* **Annotation layers:**\n",
    "\n",
    "  1. Offensive language ‚Äî offensive vs. non‚Äëoffensive\n",
    "  2. Offensiveness level ‚Äî slight, moderate, severe\n",
    "  3. Hate‚Äëspeech category ‚Äî xenophobia, racism, homophobia, sexism, religious intolerance, political partisanship, dictatorship praise, antisemitism, fatphobia\n",
    "\n",
    "ü§ó‚ÄØDatasets handles download and caching via a single command.\n",
    "The resulting `DatasetDict` includes the standard splits (e.g., `train`, `validation`, `test`), each exposing typed features and a row count.\n",
    "\n",
    "Hugging Face Datasets Hub: [https://huggingface.co/datasets](https://huggingface.co/datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 4480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 1120\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = load_dataset(\"ruanchaves/hatebr\")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='bool', id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"train\"].features[\"offensive_language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instagram_comments': ['Mais um lixo',\n",
       "  'Essa mulher √© doente.pilantra!',\n",
       "  'Vagabunda. Comunista. Mentirosa. O povo chileno nao merece uma desgra√ßa desta.'],\n",
       " 'offensive_language': [True, True, True],\n",
       " 'offensiveness_levels': [1, 3, 3],\n",
       " 'antisemitism': [False, False, False],\n",
       " 'apology_for_the_dictatorship': [False, False, False],\n",
       " 'fatphobia': [False, False, False],\n",
       " 'homophobia': [False, False, False],\n",
       " 'partyism': [False, False, True],\n",
       " 'racism': [False, False, False],\n",
       " 'religious_intolerance': [False, False, False],\n",
       " 'sexism': [False, False, True],\n",
       " 'xenophobia': [False, False, False],\n",
       " 'offensive_&_non-hate_speech': [True, True, False],\n",
       " 'non-offensive': [False, False, False],\n",
       " 'specialist_1_hate_speech': [False, False, False],\n",
       " 'specialist_2_hate_speech': [False, False, False],\n",
       " 'specialist_3_hate_speech': [False, False, False]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"test\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename target column to the default name expected by HF models: ‚Äúlabel‚Äù\n",
    "db = db.rename_column(\"offensive_language\", \"label\")\n",
    "\n",
    "# Drop columns not required for this experiment\n",
    "db = db.remove_columns(\n",
    "    [\n",
    "        \"offensiveness_levels\",\n",
    "        \"antisemitism\",\n",
    "        \"apology_for_the_dictatorship\",\n",
    "        \"fatphobia\",\n",
    "        \"homophobia\",\n",
    "        \"partyism\",\n",
    "        \"racism\",\n",
    "        \"religious_intolerance\",\n",
    "        \"sexism\",\n",
    "        \"xenophobia\",\n",
    "        \"offensive_&_non-hate_speech\",\n",
    "        \"non-offensive\",\n",
    "        \"specialist_1_hate_speech\",\n",
    "        \"specialist_2_hate_speech\",\n",
    "        \"specialist_3_hate_speech\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with BERTimbau\n",
    "\n",
    "**Model:** BERTimbau (bert‚Äëbase‚Äëportuguese‚Äëcased)\n",
    "[https://huggingface.co/neuralmind/bert-base-portuguese-cased](https://huggingface.co/neuralmind/bert-base-portuguese-cased)\n",
    "\n",
    "BERTimbau is a BERT model pre‚Äëtrained on Brazilian Portuguese.\n",
    "The corresponding tokenizer must be loaded to pre‚Äëprocess the dataset‚Äôs comment text before feeding it to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing routine that‚ÄØtokenizes the input text with the BERTimbau tokenizer and truncates any sequence that exceeds the model‚Äôs maximum input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"instagram_comments\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [00:00<00:00, 22284.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_db = db.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Padding with `DataCollatorWithPadding`\n",
    "\n",
    "`DataCollatorWithPadding` builds mini‚Äëbatches from a list of dataset items and applies padding as needed.\n",
    "With `padding=True` (default), each batch is padded only up to the length of the longest sequence **in that batch**, instead of padding every example in the entire dataset to the model‚Äôs global maximum length.\n",
    "This dynamic padding strategy reduces memory usage and speeds up training/inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Setup\n",
    "\n",
    "* **scikit‚Äëlearn:** `sklearn.metrics` supplies a wide range of model‚Äëperformance metrics.\n",
    "* **ü§ó‚ÄØevaluate:** a unified metrics API for Hugging Face workflows ‚Äî see [https://huggingface.co/docs/evaluate/a\\_quick\\_tour](https://huggingface.co/docs/evaluate/a_quick_tour).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    #compute precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    #compute accuracy score\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Fine‚Äëtune the pre‚Äëtrained BERTimbau model on the training split to classify comments as offensive or non‚Äëoffensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Setup\n",
    "\n",
    "Define an optimization function that configures a learning‚Äërate schedule along with key training hyperparameters.\n",
    "\n",
    "Further details on training a TensorFlow model with Keras:\n",
    "[https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras](https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "batches_per_epoch = len(tokenized_db[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading\n",
    "\n",
    "Load BERTimbau via `TFAutoModelForSequenceClassification`.\n",
    "\n",
    "`TFAutoModelForSequenceClassification` is a generic wrapper that instantiates the appropriate sequence‚Äëclassification model when called with `from_pretrained()` or `from_config()`.\n",
    "The sequence‚Äëclassification head is a linear layer on top of the pooled hidden states produced by the base model (BERTimbau in this case).\n",
    "\n",
    "Reference: [https://huggingface.co/docs/transformers/main/en/model\\_doc/auto#transformers.TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figs/transformer-and-head.svg\">\n",
    "https://huggingface.co/course/chapter2/2?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `prepare_tf_dataset()` to wrap each split as a TensorFlow `tf.data.Dataset`, enabling efficient, streaming‚Äëfriendly training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108923136 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108924674 (415.51 MB)\n",
      "Trainable params: 108924674 (415.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the fit method to train the model on the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 596s 2s/step - loss: 0.3506 - val_loss: 0.2092 - accuracy: 0.9205 - f1: 0.9205 - precision: 0.9211 - recall: 0.9205\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "history = model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento:\n",
      "Acur√°ria : 92.1%\n",
      "Precis√£o : 92.1%\n",
      "Revoca√ß√£o: 92.1%\n",
      "F1       : 92.1%\n",
      "Tempo de treinamento: 595.8s (or 9.9 minutes)\n"
     ]
    }
   ],
   "source": [
    "print('Treinamento:')\n",
    "print('Acur√°ria : {:.1%}'.format(history.history['accuracy'][-1]))\n",
    "#print('Acur√°ria nos dados de valida√ß√£o: {:.1%}'.format(history.history['val_accuracy'][-1]))\n",
    "print('Precis√£o : {:.1%}'.format(history.history['precision'][-1]))\n",
    "print('Revoca√ß√£o: {:.1%}'.format(history.history['recall'][-1]))\n",
    "print('F1       : {:.1%}'.format(history.history['f1'][-1]))\n",
    "print('Tempo de treinamento: {:.1f}s (or {:.1f} minutes)'.format(training_time, training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save_model = \"./\"+checkpoint+\"-fine-tuned-hatebr-db\"\n",
    "tokenizer.save_pretrained(dir_save_model)\n",
    "model.save_pretrained(dir_save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the model on the test split.\n",
    "\n",
    "In multi‚Äëclass classification, `model.predict` returns a probability vector for each class; `argmax` retrieves the index with the highest probability, yielding the predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics2(predictions, labels):\n",
    "    #compute precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    #compute accuracy score\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 35s 378ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9085714285714286,\n",
       " 'f1': 0.9084451202066525,\n",
       " 'precision': 0.9108385868137641,\n",
       " 'recall': 0.9085714285714286}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits = model.predict(tf_test_set).logits\n",
    "pred_labels = np.argmax(output_logits, axis=-1)\n",
    "compute_metrics2(pred_labels, db[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predi√ß√£o:  [1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1]\n",
      "Correto:   [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Texto:     ['Mais um lixo', 'Essa mulher √© doente.pilantra!', 'Vagabunda. Comunista. Mentirosa. O povo chileno nao merece uma desgra√ßa desta.', 'Besta quadrada.', 'Hipocritas', 'Quem tem pena √© galinha, mas ela √© uma VACA LOUCA.', 'Mande essa Bachelet plantar batata no asfalto. Puta que pariu at√© o tamborete de bordel se acha no direito de falar do Brasil.', 'Cara de pau.', 'Porque √© uma bandida', 'Tudo igual. S√≥ pensam no pr√≥prio rabo', 'Cretina!', 'Quem n√£o t√™m car√°ter √© contra o presidente Bolsonaro', 'Ser√° que algu√©m em s√£ consci√™ncia ainda vai acreditar nesta farsante? N√£o acredito em um fio do seu cabelo pintado.', 'Ela tem cara de homem n√©? Macron deve achar gata', 'Nojo dessa pilantra', 'Bando de canalhas', 'Vieja idiota.', 'Cretina!!!', 'Rid√≠cula nojenta', 'Isso √© uma realidade da imbecilidade humana, n√£o tem moral pr√° nada...']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predi√ß√£o: \", pred_labels[:20])\n",
    "print(\"Correto:  \", db[\"test\"][\"label\"][:20])\n",
    "print(\"Texto:    \", db[\"test\"][\"instagram_comments\"][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figs/transformer-pipeline.svg\">\n",
    "https://huggingface.co/course/chapter2/2?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sua barraqueira biscoitera\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenize the text and return TensorFlow tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(dir_save_model)\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logits** are the raw (unnormalized) prediction vector emitted by the classifier. In multi‚Äëclass settings, these logits are typically fed into a softmax layer to obtain a normalized probability distribution‚Äîone value per possible class.\n",
    "\n",
    "Feed the inputs through the model and return the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./neuralmind/bert-base-portuguese-cased-fine-tuned-hatebr-db were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./neuralmind/bert-base-portuguese-cased-fine-tuned-hatebr-db.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(dir_save_model)\n",
    "output_logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the predicted class from logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = np.argmax(output_logits, axis=-1)[0]\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###‚ÄØOffensiveness‚ÄëLevel Classification\n",
    "\n",
    "The model can be extended to predict the offensiveness level of each comment:\n",
    "\n",
    "* **Classes:**\n",
    "  0‚ÄØ=‚ÄØnon‚Äëoffensive‚ÄÉ|‚ÄÉ1‚ÄØ=‚ÄØslightly offensive‚ÄÉ|‚ÄÉ2‚ÄØ=‚ÄØmoderately offensive‚ÄÉ|‚ÄÉ3‚ÄØ=‚ÄØhighly offensive\n",
    "* **Target column:** ‚ÄØ`offensiveness_levels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 4480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 1120\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instagram_comments', 'offensive_language', 'offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = load_dataset(\"ruanchaves/hatebr\")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='int32', id=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"train\"].features[\"offensiveness_levels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(db[\"train\"][\"offensiveness_levels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instagram_comments': ['Mais um lixo',\n",
       "  'Essa mulher √© doente.pilantra!',\n",
       "  'Vagabunda. Comunista. Mentirosa. O povo chileno nao merece uma desgra√ßa desta.',\n",
       "  'Besta quadrada.',\n",
       "  'Hipocritas'],\n",
       " 'offensive_language': [True, True, True, True, True],\n",
       " 'offensiveness_levels': [1, 3, 3, 2, 2],\n",
       " 'antisemitism': [False, False, False, False, False],\n",
       " 'apology_for_the_dictatorship': [False, False, False, False, False],\n",
       " 'fatphobia': [False, False, False, False, False],\n",
       " 'homophobia': [False, False, False, False, False],\n",
       " 'partyism': [False, False, True, False, False],\n",
       " 'racism': [False, False, False, False, False],\n",
       " 'religious_intolerance': [False, False, False, False, False],\n",
       " 'sexism': [False, False, True, False, False],\n",
       " 'xenophobia': [False, False, False, False, False],\n",
       " 'offensive_&_non-hate_speech': [True, True, False, True, True],\n",
       " 'non-offensive': [False, False, False, False, False],\n",
       " 'specialist_1_hate_speech': [False, False, False, False, False],\n",
       " 'specialist_2_hate_speech': [False, False, False, False, False],\n",
       " 'specialist_3_hate_speech': [False, False, False, False, False]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"test\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.rename_column('offensiveness_levels', 'label')\n",
    "db = db.remove_columns(\n",
    "    [\n",
    "        'offensive_language',\n",
    "         'antisemitism', \n",
    "         'apology_for_the_dictatorship', \n",
    "         'fatphobia', \n",
    "         'homophobia', \n",
    "         'partyism', \n",
    "         'racism', \n",
    "         'religious_intolerance', \n",
    "         'sexism', \n",
    "         'xenophobia', \n",
    "         'offensive_&_non-hate_speech', \n",
    "         'non-offensive', \n",
    "         'specialist_1_hate_speech', \n",
    "         'specialist_2_hate_speech', \n",
    "         'specialist_3_hate_speech'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokeniza√ß√£o pelo Modelo BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1120 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1120/1120 [00:00<00:00, 29162.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_db = db.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NONE\", 1: \"MILD\", 2: \"MODERATE\", 3: \"SEVERE\"}\n",
    "label2id = {\"NONE\": 0, \"MILD\": 1, \"MODERATE\": 2, \"SEVERE\": 3}\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tokenized_db[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=total_train_steps*0.1, num_train_steps=total_train_steps)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=4, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_db[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108923136 \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108926212 (415.52 MB)\n",
      "Trainable params: 108926212 (415.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 557s 2s/step - loss: 0.9050 - val_loss: 0.7674 - accuracy: 0.6080 - f1: 0.3721 - precision: 0.3748 - recall: 0.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "history = model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=callbacks)\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento:\n",
      "Acur√°ria : 60.8%\n",
      "Precis√£o : 37.5%\n",
      "Revoca√ß√£o: 42.7%\n",
      "F1       : 37.2%\n",
      "Tempo de treinamento: 557.1s (or 9.3 minutes)\n"
     ]
    }
   ],
   "source": [
    "print('Treinamento:')\n",
    "print('Acur√°ria : {:.1%}'.format(history.history['accuracy'][-1]))\n",
    "print('Precis√£o : {:.1%}'.format(history.history['precision'][-1]))\n",
    "print('Revoca√ß√£o: {:.1%}'.format(history.history['recall'][-1]))\n",
    "print('F1       : {:.1%}'.format(history.history['f1'][-1]))\n",
    "print('Tempo de treinamento: {:.1f}s (or {:.1f} minutes)'.format(training_time, training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save_model = \"./\"+checkpoint+\"offensiveness_levels-fine-tuned-hatebr-db\"\n",
    "tokenizer.save_pretrained(dir_save_model)\n",
    "model.save_pretrained(dir_save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 39s 409ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6071428571428571,\n",
       " 'f1': 0.3638852354021155,\n",
       " 'precision': 0.38140584411743783,\n",
       " 'recall': 0.42275632262474366}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits = model.predict(tf_test_set).logits\n",
    "pred_labels = np.argmax(output_logits, axis=-1)\n",
    "compute_metrics2(pred_labels, db[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Seu jaguara corno manso\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir_save_model)\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./neuralmind/bert-base-portuguese-casedoffensiveness_levels-fine-tuned-hatebr-db were not used when initializing TFBertForSequenceClassification: ['dropout_151']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./neuralmind/bert-base-portuguese-casedoffensiveness_levels-fine-tuned-hatebr-db.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(dir_save_model)\n",
    "output_logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODERATE'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = np.argmax(output_logits, axis=-1)[0]\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current run shows limited convergence. Possible improvements include balancing the training set, tuning hyperparameters, and experimenting with larger or alternative model architectures to increase accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
